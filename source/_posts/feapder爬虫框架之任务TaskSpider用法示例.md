---
title: feapder爬虫框架之任务TaskSpider用法示例
date: 2024-03-27 18:39:59
tags: 爬虫
categories: 爬虫

---

<!--more-->

## 安装

精简版

```shell
pip install feapder
```

浏览器渲染版：

```shell
pip install "feapder[render]"
```

完整版：

```shell
pip install "feapder[all]"
```

三个版本区别：

1. 精简版：不支持浏览器渲染、不支持基于内存去重、不支持入库mongo
2. 浏览器渲染版：不支持基于内存去重、不支持入库mongo
3. 完整版：支持所有功能

## 使用

> TaskSpider是一款分布式爬虫，内部封装了取种子任务的逻辑，内置支持从redis或者mysql获取任务，也可通过自定义实现从其他来源获取任务

创建模板命令：`feapder create -s task_spider_test` 

请选择爬虫模板  TaskSpider

```
# -*- coding: utf-8 -*-
"""
Created on 2024-03-26 17:27:27
---------
@summary:
---------
@author: zhangmingwei01
"""

import feapder
from feapder import ArgumentParser
from items.spider_data_item import SpiderDataItem


class SpiderTest(feapder.TaskSpider):
    # 自定义数据库，若项目中有setting.py文件，此自定义可删除
    __custom_setting__ = dict(
        REDISDB_IP_PORTS="localhost:6379",
        REDISDB_USER_PASS="",
        REDISDB_DB=0,
        MYSQL_IP="localhost",
        MYSQL_PORT=3306,
        MYSQL_DB="data",
        MYSQL_USER_NAME="root",
        MYSQL_USER_PASS="pwd",
    )

    def add_task(self):
        # 加种子任务 框架会调用这个函数，方便往redis里塞任务，但不能写成死循环。实际业务中可以自己写个脚本往redis里塞任务
        self._redisdb.zadd(self._task_table, {"id": 1, "url": "https://www.baidu.com"})

    def start_requests(self, task):
        task_id = task.id
        url = task.url
        yield feapder.Request(url, task_id=task_id)

    def parse(self, request, response):
        # 提取网站title
        title = response.xpath("//title/text()").extract_first()
        print("网站标题：", title)
        # 提取网站描述
        print(response.xpath("//meta[@name='description']/@content").extract_first())
        print("网站地址: ", response.url)
        item = SpiderDataItem(**{
            "title": title,
            "url": response.url,
        })
        yield item

        # mysql 需要更新任务状态为做完 即 state=1
        # yield self.update_task_batch(request.task_id)


if __name__ == "__main__":
    # 用mysql做任务表，需要先建好任务任务表
    # spider = SpiderTest(
    #     redis_key="spider:test",  # 分布式爬虫调度信息存储位置
    #     task_table="spider_test",  # mysql中的任务表
    #     task_keys=["id", "url"],  # 需要获取任务表里的字段名，可添加多个
    #     task_state="state",  # mysql中任务状态字段
    # )

    # 用redis做任务表
    spider = SpiderTest(
        redis_key="spider:test2024",  # 分布式爬虫调度信息存储位置
        task_table="spider_test", # 任务表名
        task_table_type="redis", # 任务表类型为redis
    )

    parser = ArgumentParser(description="SpiderTest爬虫")

    parser.add_argument(
        "--start_master",
        action="store_true",
        help="添加任务",
        function=spider.start_monitor_task,
    )
    parser.add_argument(
        "--start_worker", action="store_true", help="启动爬虫", function=spider.start
    )

    parser.start()

    # 直接启动
    spider.start()  # 启动爬虫
    spider.start_monitor_task() # 添加任务

    # 通过命令行启动
    # python spider_test.py --start_master  # 添加任务
    # python spider_test.py --start_worker  # 启动爬虫
```

- 这里包含了入库的操作，在同级目录下创建*items*文件夹，新建一个 *spider_data_item.py*

```
# -*- coding: utf-8 -*-
"""
Created on 2024-03-27 11:41:57
---------
@summary:
---------
@author: zhangmingwei01
"""

from feapder import Item


class SpiderDataItem(Item):
    """
    This class was generated by feapder
    command: feapder create -i spider_data 1
    """

    __table_name__ = "spider_data"

    def __init__(self, *args, **kwargs):
        # self.id = kwargs.get('id')
        self.title = kwargs.get('title')
        self.url = kwargs.get('url')
```

- 需要先在数据库中建好表，库名为*data*，表名为*spider_data*，以下是建表语句

```
CREATE TABLE `spider_data` (
   `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
   `title` varchar(255) DEFAULT NULL,
   `url` varchar(255) DEFAULT NULL,
   PRIMARY KEY (`id`)
 ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4;
```

- 实际业务中可能无 *add_task*方法，应自己去手动添加任务到redis中

```
# 导入_redisdb
_redisdb.zadd(self._task_table, {"id": 1, "url": "https://www.baidu.com"})
```

